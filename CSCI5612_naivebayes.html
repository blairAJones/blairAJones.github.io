<!DOCTYPE html>
<html>

<head>
  <title>CSCI5612 - Naive Bayes & KNN</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
  <meta name="description" content="Naive Bayes model training and analysis on the Shopping Machine Learning project for CSCI5612.">


</head>

<body>

  <!-- Main Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="#">Blair Jones</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle active" href="projects.html" role="button" data-bs-toggle="dropdown"
              aria-expanded="false">
              Projects
            </a>
            <ul class="dropdown-menu">
              <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
              <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Subproject Tabs -->
  <div class="container mt-3">
    <ul class="nav nav-tabs">
      <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep / EDA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_clustering.html">Clustering</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_pca.html">PCA</a></li>
      <li class="nav-item"><a class="nav-link active" href="CSCI5612_naivebayes.html">Naive Bayes / KNN</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_decisiontrees.html">Decision Trees / Random Forests</a>
      </li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_regression.html">Regression</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_nn.html">Neural Nets</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
    </ul>

    <div class="mt-4">
      <h1>Naive Bayes and K-Nearest Neighbors</h1>
      <!-- Local section links -->
      <div class="section-nav mb-3">
        <a href="#overview" class="me-4">Overview</a>
        <a href="#data" class="me-4">Data</a>
        <a href="#code" class="me-4">Code</a>
        <a href="#results" class="me-4">Results</a>
        <a href="#summary" class="me-4">Summary</a>
      </div>

      <h3 id="overview">Overview</h3>
      <p>Naive Bayes is classifier model that uses probability for
        classification, and has a major assumption (thus, the “Naive” name) that
        features are conditionally independent given the class. This model uses
        the concepts from Bayes theorem:</p>
      <p><span class="math inline">\(P(K | X) = \displaystyle \frac{P(X | K)
          \cdot P(K)}{P(X)}\)</span></p>
      <p>With <span class="math inline">\(P(K | X)\)</span> representing the
        posterior probability of class <span class="math inline">\(K\)</span>
        given the data, <span class="math inline">\(X\)</span>; <span class="math inline">\(P(X | K)\)</span>
        representing the likelihood of
        the data features <span class="math inline">\(X\)</span> given the class <span class="math inline">\(K\)</span>;
        <span class="math inline">\(P(K)\)</span> is the prior probability of the
        class <span class="math inline">\(K\)</span>; and <span class="math inline">\(P(X)\)</span> is the evidence.
        Because of the feature independence assumption, the likelihood
        calculation ends up being <span class="math inline">\(\prod_i P(X_i |
          K)\)</span>. Naive Bayes can be done using binary features (Bernoulli
        NB), count features (Multinomial NB) and continuous features (Gaussian
        NB).
      </p>
      <p>K-Nearest Neighbors is a supervised learning method where data points
        are classified based on a plurality vote of its K (positive integer)
        nearest neighbors. As it is based on distance, numerical features are
        necessary and should be scaled prior to training. The choice of K can be
        tuned during training.</p>

      <h3 id="data">Data <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>Data files are found <a href="https://github.com/blairAJones/ShoppingML/tree/main/data"
          target="_blank">here</a>.</p>
      <p>For classification methods, a classifier response was created. <em>Is High Price</em>
        was added as a feature based on the product’s median price.
        For decision tree classification <a href="CSCI5612_decisiontrees.html">here</a>, another method was
        considered.</p>
      <p>To use Bernoulli Naive Bayes, binary features were selected:</p>
      <pre style="font-size: 12px;"><code class="language-python">
binary_features_iphone = ["topRatedBuyingExperience", "priorityListing", "discount_flag",
                          "unlocked"]
binary_features_lego = ["is_prime", "sponsored", "has_coupon", "top_theme"]

response = "is_high_price"
</code></pre>
      <p>As discussed in the <a href="CSCI5612_dataprep.html">Data Prep / EDA tab</a>, some of these binary
        features were from the original data, like “Top Rated Buying
        Experience” and “Priority Listing” for the eBay data, and “Is Prime”,
        “Sponsored”, “Is Small Business” and “Has Coupon” for the Amazon data.
        Others, like “Discount Flag” and “Top Club” were created to enhance the
        data exploration.</p>
      <p>For Gaussian Naive Bayes and K-Nearest Neighbors, numerical features
        were selected:</p>
      <pre style="font-size: 12px;"><code class="language-python">
num_features_iphone = ["seller.feedbackPercentage", "seller.feedbackScore", "days_listed",  
                       "seller_item_count", "model_number", "additional_image_count", "title_length"]
num_features_lego = ["rating", "ratings_total", "recent_sales_num", "discount$", 
                     "discount%"]
response = "is_high_price"
</code></pre>
      <p>Similar to the binary features, some numerical features were obtained
        from the original data, and some were created (like “Seller Item Count”,
        “Additional Image Count”, “Title Length” and “Discount$" and "%”).</p>
      <p>There is another Naive Bayes model, Multinomial, which is used for
        count data. Some of the numerical features are counts (e.g. seller item
        count) and others are more continuous (feedback score). All numerical
        features were scaled and thus the Gaussian NB was used for the
        below analysis.</p>
      <p>All data sets were split into training and testing sets (80/20
        split), for example:
      <pre style="font-size: 12px;"><code class="language-python">
from sklearn.model_selection import train_test_split
X_train_iphone, X_test_iphone, y_train_iphone, y_test_iphone = train_test_split(df_iphone[num_features_iphone], df_iphone[response],
                                                               test_size=0.20, random_state=1216)
</code></pre>
      This is a critical step in model building as it allows the comparison of metrics (e.g. accuracy, recall)
      on unseen data. It also allows the user to identify any potential overfitting of the training data (if there is a
      large
      discrepancy with training and testing metrics). </p>

      <h3 id="code">Code <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>A link to the full code for this analysis is found
        <a href="https://github.com/blairAJones/ShoppingML/blob/main/naive_bayes.ipynb" target="_blank">here</a>.
      </p>

      <h3 id="results">Results <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>For classification methods, the following metrics on the test
        set were considered along with the confusion matrix visual: accuracy, the
        percentage of correctly classified points; recall, the percentage of
        true positives (how many true positives were accurately predicted?); and
        the F1 score, which is a mix of recall and precision (how many predicted
        positives are actually positive?).</p>
      <h4 id="iphone">iPhone</h4>
      <p>For Naive Bayes analysis on the iPhone data, the following was
        noted:</p>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_nbb_iphone.png" alt="Image 1" width="500">
          <img src="images/3/cm_nbg_iphone.png" alt="Image 2" width="500">
        </p>
      </div>
      <p>All metrics improved on the Gaussian model. For the Bernoulli model,
        recall (how many true positives were accurately predicted) was worse
        than overall accuracy while for the Gaussian model, it was significantly
        better. Note that this model is for predicting price buckets and there
        is not a high risk of low recall (as there would be in medical detection
        or fraud cases).</p>
      <p>As discussed <a href="CSCI5612_pca.html">here</a>, PCA is a technique to reduce dimensionality by
        creating new features that are linear combinations of the originals. The
        new features are orthogonal to each other; therefore, non-correlated.
        Note that this does not mean independence; however, it was of interest to
        see if there were any differences in the PCA converted data.</p>
      <p style="text-align: center;"><img src="images/3/cm_nbg_pca_iphone.png" width="40%" /></p>
      <p>Accuracy improved using PCA projected data in a Gaussian NB model. This may indicate the consolidation of
        features into PCA projections reduced correlation compared with the originals.
      </p>
      <p>A KNN model was run on various k values with the following:</p>
      <p style="text-align: center;"><img src="images/3/knn_plot_iphone.png" width="40%" /></p>
      <p>With k=2, accuracy, recall and F1 score all improved over Naive Bayes. </p>

      <p style="text-align: center;"> <img src="images/3/cm_knn_iphone.png" width="40%" /></p>
      <p> After projecting into 2D with PCA, the true and predicted labels using KNN were
        plotted:</p>
      <p style="text-align: center;"><img src="images/3/knn_pca_iphone.png" width="70%" /></p>
      <h4 id="lego">Lego</h4>
      <p>For Naive Bayes analysis on the Lego data, the following was
        noted:</p>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_nbb_lego.png" alt="Image 1" width="500">
          <img src="images/3/cm_nbg_lego.png" alt="Image 2" width="500">
        </p>
      </div>
      <p>The Bernoulli model had mixed results – accuracy was poor and recall
        was significantly higher. This shows that the model thinks most items
        are true (price should be higher than median). The Gaussian model had
        more consistent metrics. For this data, the binary features are not good
        predictors by themselves.</p>

      <p>Similar to the iPhone data, a Gaussian NB using PCA projected features (5) was created.</p>
      <p style="text-align: center;"><img src="images/3/cm_nbg_pca_lego.png" width="40%" /></p>
      <p>While accuracy improved over the Gaussian NB on original features, recall percentage went down (there were more
        false negatives).
        It is unclear whether the PCA projected features improve this model; or if this data just does not fit with a
        Naive Bayes
        approach.
      </p>
      <p>A KNN model was run on various k values with the following:</p>
      <p style="text-align: center;"><img src="images/3/knn_plot_lego.png" width="40%" /></p>
      <p>With k=7, again, accuracy, recall and F1 score all improved over Naive Bayes.</p>
      <p style="text-align: center;"> <img src="images/3/cm_knn_lego.png" width="40%" /></p>
      <p>After projecting into 2D with PCA, the true and predicted labels
        using KNN were plotted:</p>
      <p style="text-align: center;"><img src="images/3/knn_pca_lego.png" width="65%" /></p>

      <h3 id="summary">Summary <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>The Naive Bayes models had mixed results. This is expected due to the
        nature of the data. Since the features are pulled from product listings that sellers post with much of their own
        supplied information, it is not reasonable to expect that there would be the necessary independence. What was
        interesting
        is that, depending on the product data, the models either under or overpredicted the positive class when
        compared to overall
        accuracy. There are differences in the feature sets of these products. </p>
      <p>K-Nearest Neighbors outperformed Naive Bayes for both data sets. This model allows for the flexibility to find
        local
        patterns in the data. The best k value was tuned and was different for both data sets (k=2 for iPhones, k=7 for
        Lego).  KNN could be explored for different classification responses (e.g. <em>Priority Listing</em> true/false). 
        Naive Bayes should likely not be pursued further in this analysis.
      </p>


    </div>
    <div style="height: 100px;"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>