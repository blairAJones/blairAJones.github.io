<!DOCTYPE html>
<html>

<head>
    <title>CSCI5612 - PCA</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
      height: 100%;
      /* ensure uniform tab height */
      border: 1px solid transparent;
      /* fix alignment shift on hover */
    }
    .nav-tabs .nav-link:hover,
    .nav-tabs .nav-link.active {
      border-color: #dee2e6 #dee2e6 #fff;
      /* fix active tab border */
      background-color: #f8f9fa;
    }
  </style>
  <meta name="description" content="Principal Component Analysis on the Shopping Machine Learning project for CSCI5612.">
</head>

<body>

    <!-- Main Navbar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">Blair Jones</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle active" href="projects.html" role="button"
                            data-bs-toggle="dropdown" aria-expanded="false">
                            Projects
                        </a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
                            <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Subproject Tabs -->
    <div class="container mt-3">
        <ul class="nav nav-tabs">
            <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep / EDA</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_clustering.html">Clustering</a></li>
            <li class="nav-item"><a class="nav-link active" href="CSCI5612_pca.html">PCA</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_naivebayes.html">Naive Bayes & KNN</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_decisiontrees.html">Decision Trees / Random
                    Forests</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_nn.html">Neural Nets</a></li>
            <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
        </ul>

        <div class="mt-4">
            <h1 class="mb-4">Principal Component Analysis</h1>
            <!-- Local section links -->
            <div class="section-nav mb-3">
                <a href="#overview" class="me-4">Overview</a>
                <a href="#data" class="me-4">Data</a>
                <a href="#code" class="me-4">Code</a>
                <a href="#results" class="me-4">Results</a>
                <a href="#summary" class="me-4">Summary</a>
            </div>

            <h3 id="overview">Overview</h3>
            <p>Principal components analysis is a technique of reducing
                dimensionality by linear transformation. The outputs of this analysis
                are called principal components which are new features that have been
                created from linear combinations of the originals. The first principal
                component (PC1) is derived as a combination of the features which
                capture the maximum variance. PCA is way to reduce dimension for
                large-dimensional datasets and allows the user to visualize the data in
                2D space by plotting PC1 / PC2.</p>
            <p>PCA uses concepts from linear algebra. Eigenvectors of the matrix
                <span class="math inline"><em>A</em><sup><em>T</em></sup><em>A</em></span>
                (similar to the covariance matrix) are directions in which the principal
                components point. The corresponding eigenvalues describe the variance
                amount that each component explains. If the first k components explain a
                sufficient amount of variance, then it is possible to reduce the
                dimension of the dataset by truncating by a limited number of PCs.
            </p>
            <figure style="text-align: center;">
                <img src="/images/2/pca_ex.png" alt="Example PCA plot plot with synthetic data" width=40% />
                <figcaption aria-hidden="true" style="font-size: 13px;">Example PCA plot plot with synthetic
                    data</figcaption>
            </figure>
            <p>PCA is also helps plotting in 2D as long as the first two PCs
                explain enough of the data.</p>
            <figure style="text-align: center;">
                <img src="/images/2/scree_ex.png" alt="Example Scree plot with synthetic data" width=40% />
                <figcaption aria-hidden="true" style="font-size: 13px;">Example Scree plot with synthetic
                    data</figcaption>
            </figure>
            <p>A scree plot is commonly used to visualize explained variance by
                component and to select a potential place to truncate the dimension
                (e.g.Â if the first two PCs explain 80%+, that may be sufficient.)</p>
            <br>
            <h3 id="data">Data <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
            <p>Principal Component Analysis (PCA) was completed on the eBay soccer
                jerseys data as well as the Amazon Lego data from <a href="CSCI5612_dataprep.html">Data Prep/EDA</a>
                and <a href="https://github.com/blairAJones/ShoppingML/tree/main/data" target="_blank">data files</a>.
            </p>
            <p>Because PCA uses variance / covariance data to create the linear
                combinations, only numerical features are considered.</p>
            <p>The following numerical features were used:</p>
            <p style="text-align: center;"><img src="/images/2/features_soccer.png" width=60% /></p>
            <p style="text-align: center;"><img src="/images/2/features_lego.png" width=60% /></p>
            <p>Because the values are in different scales, the features are scaled
                in order to properly assess variance.</p>
            <br>
            <h3 id="code">Code <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
            <p>A link to the full code for this analysis is found
                <a href="https://github.com/blairAJones/ShoppingML/blob/main/pca.ipynb" target="_blank">here</a>.
            </p>
            <br>
            <h3 id="results">Results <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
            <p>The data used in this project is not considered high-dimensional;
                there are seven numerical features for the eBay data and five for the
                Amazon data. However, PCA is needed for visualizing clusters in 2D, as
                well as identifying latent trends.</p>
            <h4 id="ebay-soccer-jerseys">eBay Soccer Jerseys</h4>
            <p>The following are the loadings (linear combinations of the original features)
                from the soccer data:</p>
            <p style="text-align: center;"><img src="/images/2/loadings_soccer.png" width=40% /></p>
            <p>The loadings represent how much the original features factor into the
                new principal components. For example, PC1 is heavily influenced
                (positively) by title length and additional image count, which both
                point to a more detailed listing. PC2 shows strong positive influence
                from a higher seller item count compared with a strong negative
                influence from days listed, so
                this may contrast higher vs. lower volume sellers. PC3 is driven by feedback
                percentage (proportion of positive feedback of total), which indicates a
                higher quality seller. The PCA loadings combine certain features to create additional ones like
                "high-quality listing", "high-volume seller" and "high-quality seller".</p>
            <p style="text-align: center;"><img src="/images/2/scree_soccer.png" width=40% /></p>
            <p>The scree plot shows that it takes five principal components to
                explain 80% of the variance in the data. The first two PCs only explain around 40%
                of the variance. Because the original data had
                only numerical seven features, this is not a meaningful reduction to
                dimension; however, as noted above, the PCA loadings provided some additional insight into
                the data.</p>
            <h4 id="amazon-lego">Amazon Lego</h4>
            <p>The following are the loadings from the Lego data:</p>
            <p style="text-align: center;"><img src="/images/2/loadings_lego.png" width=40% /></p>
            <p>PC1 appears to capture the connection with discounting and total
                ratings, whereas PC2 seems to capture discounts with negatively
                correlated recent sales, meaning items that are likely discounted to
                move. PC3 almost exclusively factors in the product rating.</p>
            <p style="text-align: center;"><img src="/images/2/scree_lego.png" width=40% /></p>
            <p>The scree and cumulative variance plots show that it takes three PCs
                to obtain 80% of variance explained.</p>
            <p style="text-align: center;"><img src="/images/2/proj_lego.png" width=40% /></p>
            <p>The projection of the data in PC1 and 2 was plotted with a color
                gradient from the original feature <em>recent sales</em>, as this feature was
                moderately positive in PC1 (left-right direction) and moderately
                negative in PC2 (up-down direction), which can be seen in the color gradient above.</p>
            <p style="text-align: center;"><img src="/images/2/biplot_lego.png" width=40% /></p>
            <p>The biplot highlights the direction of the features in PC1 and 2. For PC1,
                all features have a positive influence; however, some more than others, like discount
                and ratings totals, while PC2 has a positive influence from discounting and rating scores and negative
                influence from ratings totals and recent sales.
            </p>
            <h3 id="summary">Summary <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
            <p>Because the original data is not high-dimensional, PCA was mostly
                used to find latent connections within the features. Based on the
                loadings, it appeared to isolate high-quality listings and high-quality
                sellers (for eBay data) as well as the (negatively correlated)
                relationship between discount and recent sales in the case of Amazon
                data. These are interesting connections that will be used in future
                analysis.</p>
        </div>
        <div style="height: 100px;"></div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>