<!DOCTYPE html>
<html>

<head>
  <title>CSCI5612 - Decision Trees</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>

<body>

  <!-- Main Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="#">Blair Jones</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle active" href="projects.html" role="button" data-bs-toggle="dropdown"
              aria-expanded="false">
              Projects
            </a>
            <ul class="dropdown-menu">
              <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
              <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Subproject Tabs -->
  <div class="container mt-3">
    <ul class="nav nav-tabs">
      <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep/EDA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_clustering.html">Clustering</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_pca.html">PCA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_naivebayes.html">Naive Bayes & KNN</a></li>
      <li class="nav-item"><a class="nav-link active" href="CSCI5612_decisiontrees.html">Decision Trees / Random
          Forests</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_regression.html">Regression</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_nn.html">Neural Nets</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
    </ul>

    <div class="mt-4">
      <h1>Decision Trees</h1>
      <p>This is the decision trees section of the CSCI5612 project.</p>
      <!-- Local section links -->
      <div class="section-nav mb-3">
        <a href="#overview" class="me-4">Overview</a>
        <a href="#data" class="me-4">Data</a>
        <a href="#code" class="me-4">Code</a>
        <a href="#results" class="me-4">Results</a>
        <a href="#summary" class="me-4">Summary</a>
      </div>

      <h3 id="overview">Overview</h3>
      <p>Decision trees are a machine learning technique that create flowchart
        structures based on data splits. In order to decide how to best split
        the data, metrics like Gini impurity and entropy are used. Gini impurity
        measures node purity (are leaf nodes composed of a single class?), with
        a 0 value meaning a pure node and a 0.5 value meaning there is an even
        split between 2 classes. Entropy measures node disorder, with an evenly
        split node equaling 1.</p>
      <p>Let <span class="math inline">\(p_i\)</span> be the probability of
        class i: Gini Impurity: <span class="math inline">\(1 - \sum_i
          p_i^2\)</span></p>
      <p>Entropy: <span class="math inline">\(-\sum_i p_i \text{log}_2
          (p_i)\)</span></p>
      <p>Feature importance is an important metric in tree-based models and
        represents the decrease in node impurity based on specific features.</p>
      <p>Random Forests are a subset of a decision tree model that creates
        many trees during training, uses bootstrap sampling and chooses a random
        subset of features to train on, which reduces impacts of correlated
        features and can reduce overfitting by averaging many trees.</p>
      <p>Decision tree models can handle both numerical and (encoded)
        categorical features. There is not a distance measurement like in other
        models which only respond well to true continuous features.</p>

      <h3 id="data">Data <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>Classification responses were added to the data sets to test if the
        decision tree could accurately identify price buckets of “high”,
        “medium” and “low”. Because prices are not normally distributed, using a
        clustering technique (like seen here…) to assign these buckets was
        considered, rather than a simple quantile split.</p>
      <p>First, a simple K-means clustering was attempted on the microwaves
        data; however, the buckets ended up extremely unbalanced (108 items in
        the “low” bucket, while the medium and high buckets each had only 8
        items). Other options were explored, and eventually the K-Means
        Constrained technique was used. This technique uses K-Means but allows
        the user to put constraints on bucket size, to ensure a more even
        spread. </p>
      <p style="text-align: center;"><img src="images/3/kmc_summary.png" width="30%"/></p>
      <p>In addition to the assignment of pricing tiers, a simple
        “is_high_price” boolean variable was added, based on the median price.
        Either the pricing tier or is_high_price would then be the response
        variable to be classified.</p>
      <p>Because decision trees can handle both numerical and (encoded)
        categorical data, all features were considering in training. Categorical
        features like club name or Lego theme needed to be encoded as binary to
        work with these models.</p>

      <h3 id="code">Code <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>Code.</p>

      <h3 id="results">Results <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>For all data sets, a simple decision tree classifier was created
        first. Then a random forest model was built using hyperparameter tuning
        and cross validation. A grid of parameters was chosen, including max
        depth, minimum leaf size, and number of trees, and the “best” parameters
        based on training accuracy were then used on the test set.</p>
      <p>As with the Naive Bayes / KNN analysis, training and testing splits
        were created so metrics comparisons can be made on unseen data.</p>
      <h4 id="iphones">iPhones</h4>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_iphone.png" alt="Image 1" width="400">
          <img src="images/3/cm_rf_iphone.png" alt="Image 2" width="400">
        </p>
      </div>
      <p style="text-align: center;"><img src="images/3/fi_iphone_top10.png" width="40%"/></p>
      <p>Model number, days listed and title length had over 10% of feature
        importance.</p>
      <p>A partial dependence plot was generated based on days listed and
        title length.</p>
      <p style="text-align: center;"><img src="images/3/pdp_iphone.png" width="40%"/></p>
      <h4 id="soccer-jerseys">Soccer Jerseys</h4>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_soccer.png" alt="Image 1" width="400">
          <img src="images/3/cm_rf_soccer.png" alt="Image 2" width="400">
        </p>
      </div>
      <p style="text-align: center;"><img src="images/3/fi_soccer_top10.png" width="40%"/></p>
      <p>No individual feature represented over 10% of feature importance.</p>
      <h4 id="microwaves">Microwaves</h4>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_microwave.png" alt="Image 1" width="400">
          <img src="images/3/cm_rf_microwave.png" alt="Image 2" width="400">
        </p>
      </div>
      <p style="text-align: center;"><img src="images/3/fi_microwave.png" width="40%"/></p>
      <p>Cubic feet was overwhelmingly the most important feature,
        representing nearly 40%.</p>
      <h4 id="lego">Lego</h4>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_lego.png" alt="Image 1" width="400">
          <img src="images/3/cm_rf_lego.png" alt="Image 2" width="400">
        </p>
      </div>
      <p style="text-align: center;"><img src="images/3/fi_lego_top10.png" width="40%"/></p>
      <p>Four features had over 10% in importance, with top feature ratings
        total representing over 20%.</p>
      <p>Partial dependence plot:</p>
      <p style="text-align: center;"><img src="images/3/pdp_lego.png" width="40%"/></p>
      <p>Because the Random Forest model with the three classes did not show
        much of an improvement in the simple decision tree model, another RF
        model based on two classes was created.</p>
      <p style="text-align: center;"><img src="images/3/cm_rf_lego2.png" width="40%"/></p>
      <p>The two class model showed a significant improvement in accuracy.
        Since it’s a simple true/false “high price” classification, the
        mis-classed items may want to be investigated for potential
        overpricing/underpricing.</p>
      <p style="text-align: center;"><img src="images/3/lego_price_disc.png" width="60%"/></p>

      <h3 id="summary">Summary <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>As expected, given the combination of both numerical and categorical
        (and binary) variables, tree-based models performed fairly well on the
        classification of this data. Depending on</p>

    </div>
    <div style="height: 100px;"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>