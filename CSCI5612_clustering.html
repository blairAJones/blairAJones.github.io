<!DOCTYPE html>
<html>

<head>
  <title>CSCI5612 - Clustering</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
    }
  </style>
</head>

<body>

  <!-- Main Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="#">Blair Jones</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle active" href="projects.html" role="button" data-bs-toggle="dropdown"
              aria-expanded="false">
              Projects
            </a>
            <ul class="dropdown-menu">
              <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
              <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Subproject Tabs -->
  <div class="container mt-3">
    <ul class="nav nav-tabs">
      <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep / EDA</a></li>
      <li class="nav-item"><a class="nav-link active" href="CSCI5612_clustering.html">Clustering</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_pca.html">PCA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_naivebayes.html">Naive Bayes & KNN</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_decisiontrees.html">Decision Trees / Random Forests</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_regression.html">Regression</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_nn.html">Neural Nets</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
    </ul>

    <div class="mt-4">
      <h1>Clustering</h1>
      <!-- Local section links -->
      <div class="section-nav mb-3">
        <a href="#overview" class="me-4">Overview</a>
        <a href="#data" class="me-4">Data</a>
        <a href="#code" class="me-4">Code</a>
        <a href="#results" class="me-4">Results</a>
        <a href="#summary" class="me-4">Summary</a>
      </div>

      <h3 id="overview">Overview</h3>
      <p>K-Means and hierarchical clustering are unsupervised machine learning
        techniques which group data together based on similarity metrics.
        K-Means uses a pre-specified number of clusters (k) to partition the data 
        by minimizing the within cluster sum of squares (squared Euclidean distances)
        and updating cluster centroids using the expectation-maximization algorithm:
        assign each point to nearest center then update each cluster's centroid, until 
        convergence has been obtained.</p>
        
        <p>Hierarchical
        clustering grows a hierarchy of clusters from the bottom-up.  The model starts 
        with each point as its own cluster and merges clusters based on a linkage method 
        and distance formula. Both
        methods allow the user to identify underlying patterns in the data
        across multiple features and can assist with reducing the dimension of
        large datasets.</p>
      <figure style="text-align: center;">
        <img src="/images/2/kmeans_ex.png" alt="Example of K-Means using synthetic data" width=40% />
        <figcaption style="font-size: 13px;">Example of K-Means using synthetic
          data</figcaption>
      </figure>
      <figure style="text-align: center;">
        <img src="/images/2/hclust_ex.png" alt="Example of H-Clustering using synthetic data" width=40% />
        <figcaption style="font-size: 13px;">Example of H-Clustering using synthetic
          data</figcaption>
      </figure>
      <p>For this particular project, various quantitative features are
        analyzed via clustering to identify which items naturally cluster
        together and how their features are related. For example, on the eBay
        data, is there a relationship between the seller’s ratings and
        additional images posted on the listings? </p>
      <p style="text-align: center;"><img src="/images/2/addimages_feedback.png" width=30% /></p>
      <p>Details are discussed below.</p>
      <br>
      <h3 id="data">Data <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>The dataframes created from 
        <a href="CSCI5612_dataprep.html">Data Prep/EDA</a> and <a href="https://github.com/blairAJones/ShoppingML/tree/main/data" target="_blank">data files</a>
        continue to be analyzed.</p>
      <p>For the eBay data, the numerical features used in this analysis were
        as follows:</p>
      <p><img src="/images/2/features_iphones.png" width=60%/></p>
      <p>The seller feedback score represents the count of feedback received
        and the percentage represents the proportion of positive feedback out of
        total.</p>
      <p>For the Amazon data, the numerical features used in this analysis
        were as follow:</p>
      <p><img src="/images/2/features_micro.png" width=60%/></p>
      <p>There are fewer quantitative features for this data and additional
        filtering needed to be completed as not all listings had a cubic feet
        (microwave size) description in the title.</p>
      
      <br>
      <h3 id="code">Code <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>A link to the full code for this analysis is found 
        <a href="https://github.com/blairAJones/ShoppingML/blob/main/clustering.ipynb" target="_blank">here</a>. </p>
      
      <br>
      <h3 id="results">Results <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>The following clustering results are noted analyzing the eBay iPhones data and 
        Amazon microwaves data.</p>
      <h4 id="ebay-iphones-data">eBay iPhones Data</h4>
      <h5 id="k-means">K-Means</h5>
      <p>On the eBay iPhone data, a K-Means elbow plot was created to identify
        a choice for k number of clusters.  This plot is meant to find at which k 
      is the decrease in the inertia (within-cluster sum of squares) only marginal.</p>
      <p style="text-align: center;"><img src="/images/2/elbow_iphone.png" width=40%/></p>
      <p>There is not a clear point in which the plot “bends”; although it
        does appear that around k=4 or 5, there is a decrease in the slope, but
        does seem to increase slightly again.</p>
      <p>Then, a silhouette plot was created to assist with the selection of
        k.  The silhouette method measures how similar a data point is within its 
        own cluster (from -1 to 1).  The average measure per k value is then plotted, 
        where the higher values represent data that is more similar to its clusters. </p>
      <p style="text-align: center;"><img src="/images/2/silh_iphone.png" width=40%/></p>
      <p>Based on the peak at 4, k=4 was chosen for further
        analysis.  Although k=11 looks to be slightly higher; a smaller k should be 
        better for interpretability.  </p>
      <p style="text-align: center;"><img src="/images/2/kclust_sum.png" width=60%/></p>
      <p>The above cluster summary shows the counts and mean feature values by
        cluster number.  While the clusters are not balanced, there 
        are some trends noted; for example, higher quantity and volume sellers in 
        cluster 2 compared with 3, and higher additional images and title 
        lengths in cluster 1.</p>
      <p>To plot the full data using the cluster colors, the dimensions were
        reduced to two via PCA projection (see <a href="CSCI5612_pca.html">PCA</a> tab for additional information).</p>
      <p style="text-align: center;"><img src="/images/2/kmeans_wPCA_iphone.png" width=40%/></p>
      <p>Unfortunately, for this data, the reduction to two dimensions does
        not assist with the visualization of the clusters.  The first two PCs explain
        roughly 50% of the variance.</p>
      <p>Select features were plotted against each other to identify
        interesting patterns. One example was seller feedback vs.additional image count:</p>
      <p style="text-align: center;"><img src="/images/2/feedback_vs_image_ct.png" width=40%/></p>
      <p>Animated clustering process:</p>
      <p style="text-align: center;"><img src="/images/2/kmeans.gif" width=50%/></p>
      <p>The centroids and clusters converged after six iterations.</p>
      <h5 id="hierarchical">Hierarchical</h5>
      <p>For hierarchical clustering, one needs to consider a linkage method
        when building the clusters up. Some options include Ward, which
        minimizes the within cluster variances as they are built; complete,
        which considers the maximum distance between any pair of points within a
        cluster; and average, which considers the average distance of the points
        in a cluster. These can use Euclidean distance to measure; another option
        would be to consider cosine similarity (using cosine of angles between points, -1 to 1) 
        as the distance ("similarity") measure, along
        with a linkage function (note that Ward uses Euclidean only). So, Ward and
        complete linkage with Euclidean distance along with complete linkage and
        cosine similarity distance were chosen to create dendograms.</p>
      <p style="text-align: center;"><img src="/images/2/dendogram_iphone.png" width=100%/></p>
      <p><img src="/images/2/hclust_counts.png" width=20%/></p>
      <p>Based on the dendogram and cluster counts, k=6 was selected with 
        the complete linkage using cosine similarity distance, as it appears 
        to have the most balanced clusters.</p>
      <p>These clusters were plotted via PCA projection.</p>
      <p style="text-align: center;"><img src="/images/2/hclust_iphone.png" width=40%/></p>
      <p>Compared with K-Means, there appears to be more evidence of clusterings using this method, although
        there is still significant overlap amongst the clusters.
      </p>
      <h4 id="amazon-microwaves-data">Amazon Microwaves Data</h4>
      <h5 id="k-means-1">K-Means</h5>
      <p>The microwaves data from Amazon was selected to perform similar
        K-means clustering analysis. An elbow plot and silhouette plot were
        created to assist with the choice of k.</p>
      <p style="text-align: center;"><img src="/images/2/elbow_micro.png" width=40%/></p>
      <p style="text-align: center;"><img src="/images/2/silh_micro.png" width=40%/></p>
      <p>Based on both charts, k = 3 was selected for further analysis.</p>
      <p style="text-align: center;"><img src="/images/2/kmeans_wPCA_micro.png"width=40% /></p>
      <p>The PCA projection shows some small evidence of clusters; however, there remains
        significant overlap between the groups.  The first two
        principal components contributed more explained variance (63%) compared
        with the iPhones data (50%).</p>
      <p>Two select features were plotted, cubic feet (size) vs. ratings
        totals, and achieved convergence within 5 iterations.</p>
      <p style="text-align: center;"><img src="/images/2/kmeans_micro.gif" width=50%/></p>
      <h3 id="summary">Summary <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>Ultimately, clustering found some interesting connections but it
        remained difficult to identify clear clusters in this data. It is
        possible that the data is just not naturally clustered, or that the
        numerical features do not tell the whole story of these datasets, and
        categorical feature analysis is required as well.  Eventually, investigating other clustering 
        methods that allow for categorical features, such as K-Prototypes and K-Modes, will be
        pursued.  </p>
    </div>
    <div style="height: 100px;"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>