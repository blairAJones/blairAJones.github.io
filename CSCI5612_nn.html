<!DOCTYPE html>
<html>

<head>
  <title>CSCI5612 - Neural Networks</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
      height: 100%;
      /* ensure uniform tab height */
      border: 1px solid transparent;
      /* fix alignment shift on hover */
    }

    .nav-tabs .nav-link:hover,
    .nav-tabs .nav-link.active {
      border-color: #dee2e6 #dee2e6 #fff;
      /* fix active tab border */
      background-color: #f8f9fa;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
  <meta name="description"
    content="Neural Network model training and analysis on the Shopping Machine Learning project for CSCI5612.">
</head>

<body>

  <!-- Main Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="#">Blair Jones</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle active" href="projects.html" role="button" data-bs-toggle="dropdown"
              aria-expanded="false">
              Projects
            </a>
            <ul class="dropdown-menu">
              <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
              <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Subproject Tabs -->
  <div class="container mt-3">
    <ul class="nav nav-tabs">
      <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep / EDA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_clustering.html">Clustering</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_pca.html">PCA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_naivebayes.html">Naive Bayes & KNN</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_decisiontrees.html">Decision Trees / Random Forests</a>
      </li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
      <li class="nav-item"><a class="nav-link active" href="CSCI5612_nn.html">Neural Nets</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
    </ul>

    <div class="mt-4">
      <h1 class="mb-4">Neural Networks</h1>
      <!-- Local section links -->
      <div class="section-nav mb-3">
        <a href="#overview" class="me-4">Overview</a>
        <a href="#data" class="me-4">Data</a>
        <a href="#code" class="me-4">Code</a>
        <a href="#results" class="me-4">Results</a>
        <a href="#summary" class="me-4">Summary</a>
      </div>

      <h3 id="overview">Overview</h3>
      <p>Neural Networks are a sophisticated, supervised machine learning
        methods that loosely mimic activity in the human brain. Neural nets
        connect nodes with edges; inputs to each layer are parameters that are
        transformed using a non-linear activation function, like sigmoid or ReLU, and outputs then go
        to the next layer. The inputs are adjusted by weights which are learned
        during training.</p>

      <figure style="text-align: center; margin: 0;">
          <img src="images/5/nn_graph.png" alt="Example NN" width="450">
          <figcaption style="font-size: 11px; margin-top: 4px;">
            Example NN with 6 inputs, 1 hidden layer of 4 neurons with activation, and 3 outputs
          </figcaption>
        </figure>  
      <p>Neural nets can be used for both classification and regression
        problems. Both types were used in the following analysis.</p>
      <p>Pytorch was used for all modeling.</p>
      <br>
      <h3 id="data">Data <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>Data files are found <a href="https://github.com/blairAJones/ShoppingML/tree/main/data"
          target="_blank">here</a>.</p>
      <p>Similar to other supervised methods, a training and testing set were
        split before model building. Another split of the training set, called
        the validation set, was created as well, which was used during training
        to assess different hyperparameters like number of layers, learning
        rate, etc., similar to k-fold cross-validation. Then, the final model
        (based on best performance during training) was trained on the combined
        train/validation set, and then evaluated on the test set. For
        classification model, accuracy was used as the evaluation metric. For
        the regression model, RMSE (root mean squared error) was used as the
        metric.</p>
      <p>RMSE = <span class="math inline">\(\displaystyle \sqrt{\sum_{i=1}^n \frac{(\hat{y}_i-y_i)^2}{n}}\)</span></p>
      <p>Unlike SVM or unsupervised clustering, which depends on vector
        distance, neural nets can use other types of features like binary or
        one-hot-encoded categorical features (distance in these cases is not
        meaningful). The continuous numerical features were scaled and then
        binary and OHE categorical features were included to create a large
        feature base from which the model can learn.</p>
      <p>Pytorch uses a torch.tensor data type for its main data structure.
        Then, a TensorDataset and DataLoader are needed to run the models. The
        TensorDatasets wrap the feature and response tensors into an object.
        Then, the DataLoader creates an iterable over the TensorDataset which
        helps with batching and shuffling as needed in training.</p>
      <br>
      <h3 id="code">Code <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>A link to the full code for this analysis is found
        <a href="https://github.com/blairAJones/ShoppingML/blob/main/nn.ipynb" target="_blank">here</a>.
      </p>
      <p>Pytorch was used. A simple NN
        class as seen below was used; the output layer should be modified
        depending on if itâ€™s a classification or regression task.
      </p>
      <pre style="font-size: 12px;"><code class="language-python">
class SimpleNN(nn.Module):
    def __init__(self, input_dim, units, dropout, n_layers, num_classes):
        super(SimpleNN, self).__init__()
        layers = []
        # First layer
        layers.append(nn.Linear(input_dim, units))
        layers.append(nn.BatchNorm1d(units)) 
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout))
        # Hidden layers
        for _ in range(n_layers - 1):
            layers.append(nn.Linear(units, units))
            layers.append(nn.BatchNorm1d(units)) 
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
        # Output layer
        layers.append(nn.Linear(units, num_classes))  #update for regression
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        return self.net(x)
</code></pre>
      <p>Then, a training and validation function was created which returned
        validation set results after looping through a parameter grid like this
        one:</p>
      <pre style="font-size: 12px;"><code class="language-python">
param_grid = {
    'units': [32, 64],
    'dropout': [0.3, 0.4, 0.5],
    'learning_rate': [0.0005, 0.001, 0.005, 0.01],
    'n_layers': [1, 2, 3]
}
</code></pre>
      <p>Units are the number of neurons per layer and can handle more complex
        data, but could also lead to overfitting. The dropout rate randomly
        chooses a percentage of neurons to set to zero during training to
        prevent overfitting. The learning rate measures the size of the steps
        the optimizer uses to update weights after each batch. The number of
        layers represents the number of hidden layers in the model.</p>
      <p>The best model based on loss was then re-trained on the combined
        train/validation set and then evaluated on the test set (previously
        unseen data).</p>
      <p>An example of a final model is as follows:</p>
      <pre style="font-size: 12px;"><code class="language-python">
final_model = SimpleNN(
    input_dim=X_train_tensor_iphone.shape[1],
    units=32,
    dropout=0.3,
    n_layers=2,
    num_classes=3)
  </code> </pre>
      <p>The softmax function is used in the output layer for probabilities and ReLU 
         is used as the activation function in the hidden layers.  For classification tasks 
        cross entropy loss is the criterion; for regression, MSE is used.</p>
      <br>
      <h3 id="results">Results <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>The iPhone and Soccer data was analyzed using neural nets. Both
        classification, using the multi-class pricing tiers of low, medium,
        high, seen <a href="CSCI5612_decisiontrees.html">here</a>, and regression. In addition,
        a baseline multiple
        linear regression with the same features was created to compare with the
        regression neural net.</p>
      <p>Recall the multi-class pricing tiers, which were also used in
        decision tree modeling:</p>
      <p style="text-align: center;"><img src="images/5/pricing_tiers.png" width="50%" /></p>
      <h4 id="iphone---classification">iPhone - Classification</h4>
      <p>As mentioned above, multiple models were trained and validated based
        on a parameter grid, with the <em>best</em> model selected based on
        lowest validation loss. Then, the best model hyperparameters were
        trained on the combined training/validation data, with the following
        results seen.</p>
      <p>Best hyperparams: {'units': 32, 'dropout': 0.3, 'lr': 0.005, 'n_layers': 1, 'val_loss': 0.3963206458476282, 'val_acc': 0.8870967741935484}</p>
      <p>Given the large number of parameters (42 input features given the one-hot-encoding) 
        and 32 neurons selected by cross-validation, an example of the NN architecture could not be 
      drawn.  The following are the outputs of some of the model parameters after training:</p>
      <p>This is the first layer, 42 parameters and 32 neurons.</p>
      <p style="text-align: center;"><img src="images/5/input.png" width="50%"></p>
      <p>The output has 3 values for the multi-class classification.</p>
      <p style="text-align: center;"><img src="images/5/output.png" width="50%"></p>
      <p>This is the final model architecture.</p>
      <p style="text-align: center;"><img src="images/5/final_model.png" width="50%"></p>

      <p style="text-align: center;"><img src="images/5/training_progress.gif" width="50%"></p>
      <p>The above animation shows the loss and accuracy throughout the
        training epochs. A good modelâ€™s loss decreases over time. And accuracy
        increases. There are cases when these trends reverse, and thus sometimes
        is it wise to build in a stopping point in your model which stops when
        loss ceases to decrease.</p>
       <p>
        Here are the last 5 epochs:<br>
        Epoch 46, Loss: 0.2081, Accuracy: 0.9318<br>
        Epoch 47, Loss: 0.1956, Accuracy: 0.9221<br>
        Epoch 48, Loss: 0.2146, Accuracy: 0.9188<br>
        Epoch 49, Loss: 0.2127, Accuracy: 0.9156<br>
        Epoch 50, Loss: 0.2189, Accuracy: 0.9026
       </p> 
      <p style="text-align: center;"><img src="images/5/cm_nn_iphone.png" width="50%" /></p>
      <p>The neural net model performed well on the iPhone data, with 87%
        accuracy. No mis-classes from the high/low buckets were noted.</p>
      <h4 id="iphone---regression">iPhone - Regression</h4>
      <p>A regression model was then built to predict price based on the same
        features used in the classification task. RMSE was the evaluation metric
        used on the test set. In addition to a model trained on original price,
        another model using log price was built to see if there was an
        improvement in RMSE, which is in the original unit of measure, dollars.
        After training and predicting on log price, the predictions were then
        exponentiated back to dollars for comparison.</p>
      <p>Original Price - Test RMSE: 94.48<br>
        Log Price - Test RMSE (log scale):0.3469<br>
        Log Price - Test RMSE (original scale): 209.1262</p>
      <p>The model built using the log price was significantly worse than the
        one using the original price. At this time, it is unclear what is
        causing the large variance.</p>
      <p>The top ten deltas from predicted price to actual price were sorted
        and reviewed, with some potential flags for overpriced /
        underpriced.</p>
      <p style="text-align: center;"><img src="images/5/iphone_over_under_reg.png" width="70%" /></p>
      <p>As a baseline, multiple linear regression models (original and log
        price) using the same features were created, with the following
        results:</p>
      <p>Adjusted <span class="math inline">\(R^2\)</span> was strong for both
        models; with 83.3% and 85% of variance explained by the models,
        respectively.</p>
      <p>Original Price - Test RMSE: 100.3040<br>
        Log Price - Test RMSE: 0.1479<br>
        Log Price Converted - Test RMSE: 105.1955</p>
      <p>Test RMSE for original price was somewhat in-line with the neural net; the log
        price model MLR model performed significantly better than the NN model.
        Ultimately, using log price instead of original dollars did not show
        improvement.</p>
      <h4 id="soccer---classification">Soccer - Classification</h4>
      <p>The same procedure for training and validating the NN model was used
        on the soccer data set.</p>
      <p>Best hyperparams: {'units': 64, 'dropout': 0.5, 'lr': 0.005, 'n_layers': 3, 'val_loss': 0.6605579302090556, 'val_acc': 0.6722689075630253}
      </p>
      <p style="text-align: center;"><img src="images/5/cm_nn_soccer.png" width="50%" /></p>
      <p>Overall, the model performed well, with only 3 data points that were
        mis-classed between high and low.</p>
      <p style="text-align: center;"><img src="images/5/soccer_over_under_class.png" width="70%" /></p>
      <p>Some potential under and over priced items were flagged based on the
        high/low mis-classes. Additional research into these items could be
        completed to determine if additional flagging of words to be turned into
        potential features could be done.</p>
      <h4 id="soccer---regression">Soccer - Regression</h4>
      <p>The regression neural net model was then trained. Test RMSE was
        $23.69. Similar to the iPhone data, original price was converted to log
        scale and then trained. No improvement in RMSE was noted; however, the
        results were more in-line with the original price than with the iPhone
        data.</p>
      <p>Test RMSE: 23.69<br>
        Test RMSE (log scale): 0.3409<br>
        Test RMSE (original scale): 25.1927</p>
      <p style="text-align: center;"><img src="images/5/soccer_over_under_reg.png" width="70%" /></p>
      <p>The top ten price prediction misses were sorted.</p>
      <p>Two multiple linear regression models were then trained for
        comparison using the same features. Different from the iPhone data, the
        neural net models on the soccer data performed better than basic MLR
        models.</p>
      <p>Adj. R-squared (Original): 0.549<br>
        Adj. R-squared (Log Price): 0.553<br>
        Original Price - Test RMSE: 29.8098<br>
        Log Price - Test RMSE: 0.3825<br>
        Log Price Converted - Test RMSE: 28.5054</p>
      <br>
      <h3 id="summary">Summary <a href="#top" style="font-size: 0.5em;">[Top]</a></h3>
      <p>Neural nets offer more flexibility for the eBay listings data as more
        than just continuous numerical features can be used, unlike other
        supervised models such as k-nearest neighbors and support vector
        machines. In addition, neural nets can be used for both multi-class
        classification and regression, which applied in this case. One needs to
        be particular about data pre-processing; however, as any NaN fields will
        cause the model to break (compared with Random Forest which can handle
        missing data). Given the abundance of non-continuous features, neural
        networks are a good choice for this data.</p>
      <p>Multiple linear regression was also run as a baseline for both the
        iPhone and Soccer datasets. Interestingly, the neural network and MLR 
        performed in-line with the iPhone data. The MLR models (original price and log price) both had high
        adjusted <span class="math inline">\(R^2\)</span> (83.3% and 85%,
        respectively). The soccer dataâ€™s NN models performed better than the
        MLR. One could suppose that there is an inherent difference in the
        nature of the data, one more linear and one more flexible, which leads
        to different models performing differently. Given the flexibility of
        neural networks, additional adjustments can be made in the future to
        further refine the models.</p>
    </div>
    <div style="height: 100px;"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>