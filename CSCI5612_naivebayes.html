<!DOCTYPE html>
<html>

<head>
  <title>CSCI5612 - Naive Bayes & KNN</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    /* wrap tab labels */
    .nav-tabs .nav-link {
      white-space: normal !important;
      word-break: break-word;
      max-width: 160px;
      display: block;
      text-align: center;
    }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</head>

<body>

  <!-- Main Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="#">Blair Jones</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="about.html">About</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle active" href="projects.html" role="button" data-bs-toggle="dropdown"
              aria-expanded="false">
              Projects
            </a>
            <ul class="dropdown-menu">
              <li><a class="dropdown-item active" href="CSCI5612.html">CSCI5612</a></li>
              <li><a class="dropdown-item" href="coming_soon.html">Coming Soon</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Subproject Tabs -->
  <div class="container mt-3">
    <ul class="nav nav-tabs">
      <li class="nav-item"><a class="nav-link" href="CSCI5612.html">Introduction</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_dataprep.html">Data Prep/EDA</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_clustering.html">Clustering</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_pca.html">PCA</a></li>
      <li class="nav-item"><a class="nav-link active" href="CSCI5612_naivebayes.html">Naive Bayes / KNN</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_decisiontrees.html">Decision Trees / Random Forests</a>
      </li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_svm.html">SVM</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_regression.html">Regression</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_nn.html">Neural Nets</a></li>
      <li class="nav-item"><a class="nav-link" href="CSCI5612_conclusion.html">Conclusion</a></li>
    </ul>

    <div class="mt-4">
      <h1>Naive Bayes and K-Nearest Neighbors</h1>
      <!-- Local section links -->
      <div class="section-nav mb-3">
        <a href="#overview" class="me-4">Overview</a>
        <a href="#data" class="me-4">Data</a>
        <a href="#code" class="me-4">Code</a>
        <a href="#results" class="me-4">Results</a>
        <a href="#summary" class="me-4">Summary</a>
      </div>

      <h3 id="overview">Overview</h3>
      <p>Naive Bayes is classifier model that uses probability for
        classification, and has a major assumption (thus, the “Naive” name) that
        features are conditionally independent given the class. This model uses
        the concepts from Bayes theorem:</p>
      <p><span class="math inline">\(P(K | X) = \displaystyle \frac{P(X | K)
          \cdot P(K)}{P(X)}\)</span></p>
      <p>With <span class="math inline">\(P(K | X)\)</span> representing the
        posterior probability of class <span class="math inline">\(K\)</span>
        given the data, <span class="math inline">\(X\)</span>; <span class="math inline">\(P(X | K)\)</span>
        representing the likelihood of
        the data features <span class="math inline">\(X\)</span> given the class <span class="math inline">\(K\)</span>; 
        <span class="math inline">\(P(K)\)</span> is the prior probability of the
        class K; and <span class="math inline">\(P(X)\)</span> is the evidence.
        Because of the feature independence assumption, the likelihood
        calculation ends up being <span class="math inline">\(\prod_i P(X_i |
          K)\)</span>. Naive Bayes can be done using binary features (Bernoulli
        NB), count features (Multinomial NB) and continuous features (Gaussian
        NB).</p>
      <p>K-Nearest Neighbors is a supervised learning method where data points
        are classified based on a plurality vote of its K (positive integer)
        nearest neighbors. As it is based on distance, numerical features are
        necessary and should be scaled prior to training. The choice of K can be
        tuned during training.</p>

      <h3 id="data">Data <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>For classification methods, a classifier response was created. “Is
        High Price” was added as a feature based on the product’s median price.
        For decision tree classification here, another method was
        considered.</p>
      <p>To use Bernoulli Naive Bayes, binary features were selected:</p>
      <pre style="font-size: 11px;"><code class="language-python">
binary_features_iphone = ["topRatedBuyingExperience", "priorityListing", "discount_flag",
                          "unlocked"]
binary_features_lego = ["is_prime", "sponsored", "has_coupon", "top_theme"]

response = "is_high_price"
</code></pre>
      <p>As discussed in the Data Prep / EDA tab, some of these binary
        features were from the original data, like “Top Rated Buying
        Experience”, “Priority Listing” for the eBay data, and “Is Prime”,
        “Sponsored”, “Is Small Business” and “Has Coupon” for the Amazon data.
        Others, like “Discount Flag” and “Top Club” were created to enhance the
        data exploration.</p>
      <p>For Gaussian Naive Bayes and K-Nearest Neighbors, numerical features
        were selected:</p>
      <pre style="font-size: 11px;"><code class="language-python">
num_features_iphone = ["seller.feedbackPercentage", "seller.feedbackScore", "days_listed",  
                       "seller_item_count", "model_number", "additional_image_count", "title_length"]
num_features_lego = ["rating", "ratings_total", "recent_sales_num", "discount$", 
                     "discount%"]
response = "is_high_price"
</code></pre>
      <p>Similar to the binary features, some numerical features were obtained
        from the original data, and some were created (like “Seller Item Count”,
        “Additional Image Count”, “Title Length” and “Discount$ and %”).</p>
      <p>There is another Naive Bayes model, Multinomial, which is used for
        count data. Some of the numerical features are counts (e.g. seller item
        count) and others are more continuous (feedback score). All numerical
        features were scaled and thus the Gaussian NB was used for the
        above.</p>

      <h3 id="code">Code <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>Code.</p>

      <h3 id="results">Results <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>All data sets were split into training and testing sets (80/20
        split). For classification methods, the following metrics on the test
        set were considered along with the confusion matrix image: accuracy, the
        percentage of correctly classified points; recall, the percentage of
        true positives (how many true positives were accurately predicted?); and
        the F1 score, which is a mix of recall and precision (how many predicted
        positives are actually positive?).</p>
      <h4 id="iphone">iPhone</h4>
      <p>For Naive Bayes analysis on the iPhone data, the following was
        noted:</p>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_nbb_iphone.png" alt="Image 1" width="400">
          <img src="images/3/cm_nbg_iphone.png" alt="Image 2" width="400">
        </p>
      </div>
      <p>All metrics improved on the Gaussian model. For the Bernoulli model,
        recall (how many true positives were accurately predicted) was worse
        than overall accuracy while for the Gaussian model, it was significantly
        better. Note that this model is for predicting price buckets and there
        is not a high risk of low recall (as there would be in medical detection
        or fraud cases).</p>
      <p>A KNN model was run on various k values with the following:</p>
      <p style="text-align: center;"><img src="images/3/knn_plot_iphone.png" width="40%"/></p>
      <p>With k=2, accuracy, recall and F1 score all improved over Naive Bayes. </p>
       
      <p style="text-align: center;"> <img src="images/3/cm_knn_iphone.png" width="40%"/></p>
      <p> After projecting into 2D with PCA, the true and predicted labels using KNN were
        plotted:</p>
      <p style="text-align: center;"><img src="images/3/knn_pca_iphone.png" width="65%"/></p>
      <h4 id="lego">Lego</h4>
      <p>For Naive Bayes analysis on the Lego data, the following was
        noted:</p>
      <div style="display: flex; justify-content: center; gap: 10px; margin-top: 20px;">
        <p><img src="images/3/cm_nbb_lego.png" alt="Image 1" width="400">
          <img src="images/3/cm_nbg_lego.png" alt="Image 2" width="400">
        </p>
      </div>
      <p>The Bernoulli model had mixed results – accuracy was poor and recall
        was significantly higher. This shows that the model thinks most items
        are true (price should be higher than median). The Gaussian model had
        more consistent metrics. For this data, the binary features are not good
        predictors by themselves.</p>
      <p>As discussed here, PCA is a technique to reduce dimensionality by
        creating new features that are linear combinations of the originals. The
        new features are orthogonal to each other; therefore, non-correlated.
        Note that this does not mean independence.</p>
      <p>A Gaussian NB on PCA features (5) was created.</p>
      <p style="text-align: center;"><img src="images/3/cm_nbg_pca_lego.png" width="40%"/></p>
      <p>A KNN model was run on various k values with the following:</p>
      <p style="text-align: center;"><img src="images/3/knn_plot_lego.png" width="40%"/></p>
      <p>With k=7, again, accuracy, recall and F1 score all improved over Naive Bayes.</p>
      <p style="text-align: center;"> <img src="images/3/cm_knn_lego.png" width="40%"/></p>
      <p>After projecting into 2D with PCA, the true and predicted labels
        using KNN were plotted:</p>
      <p style="text-align: center;"><img src="images/3/knn_pca_lego.png" width="65%"/></p>

      <h3 id="summary">Summary <a href="#top" style="font-size: 0.6em;">[Top]</a></h3>
      <p>The Naive Bayes models had mixed results. This is expected due to the
        nature of the data. Since these are product listings</p>


    </div>
    <div style="height: 100px;"></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>